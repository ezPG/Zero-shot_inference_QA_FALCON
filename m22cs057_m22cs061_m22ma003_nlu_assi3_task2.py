# -*- coding: utf-8 -*-
"""M22CS057_M22CS061_M22MA003_NLU_Assi3_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KjHuhjTaUep92GnSg7--w97-2o9BXpGx
"""

!pip install -q transformers evaluate torchmetrics

# import libraries
import torch
import nltk
import evaluate
import pandas as pd
from transformers import AutoTokenizer
from transformers import AutoModelForQuestionAnswering
from torchmetrics.text import BLEUScore
from torchmetrics.text.rouge import ROUGEScore
from torchmetrics.text import SacreBLEUScore
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from transformers import AutoModelForQuestionAnswering
from nltk.translate.gleu_score import sentence_gleu



import json
from datasets import load_dataset

# device config
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')

"""# Evaluation metrics"""

bleu = BLEUScore()
rouge = ROUGEScore()
sacre_bleu = SacreBLEUScore()

"""# Model config"""

# load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("Falconsai/question_answering_v2")
# inputs = tokenizer(question, context, return_tensors="pt")

model = AutoModelForQuestionAnswering.from_pretrained("Falconsai/question_answering_v2")
model = model.to(device)

"""# PrivacyQA Dataset"""

# read dataset
test_privacy_df = pd.read_csv("/content/policy_test_data.csv", sep="\t")
test_privacy_df = test_privacy_df[['Query', 'Segment']]

test_privacy_df = test_privacy_df.drop_duplicates(keep=False)

# mkae datasetclass
class CustomDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=64):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question = self.data.iloc[idx]['Query']
        context = self.data.iloc[idx]['Segment']

        # Tokenize question and context with padding and truncation
        inputs = self.tokenizer(question, context, return_tensors="pt", padding=True, truncation=True, max_length=self.max_length)

        return inputs, context

# Create an instance of your custom dataset
custom_dataset = CustomDataset(test_privacy_df, tokenizer)

# make dataloader
BATCH_SIZE = 1
dataloader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

bleu_score = 0
sacre_bleu_score = 0
gleu_score = 0

rouge1_fmeasure = 0
rouge1_precision = 0
rouge1_recall = 0

rouge2_fmeasure = 0
rouge2_precision = 0
rouge2_recall = 0

rougeL_fmeasure = 0
rougeL_precision = 0
rougeL_recall = 0



model = model.to(device)
j = 0
for batch, context in dataloader:
    with torch.no_grad():
        batch['input_ids'] = batch['input_ids'].squeeze(dim=0)
        batch['attention_mask'] = batch['attention_mask'].squeeze(dim=0)

        batch = batch.to(device)
        outputs = model(**batch)
        answer_start_indices = outputs.start_logits.argmax(dim=1)
        answer_end_indices = outputs.end_logits.argmax(dim=1)
        for i in range(BATCH_SIZE):

            start_idx = answer_start_indices[i].item()
            end_idx = answer_end_indices[i].item()
            predict_answer_tokens = batch.input_ids[i, start_idx : end_idx + 1]
            predicted_answer = tokenizer.decode(predict_answer_tokens)


            bleu_results = bleu([predicted_answer], [[context[0]]])
            rouge_result = rouge(predicted_answer, context[0])
            sacre_bleu_result = sacre_bleu([predicted_answer], [[context[0]]])
            gleu_score_result = sentence_gleu(predicted_answer.split(), context[0].split())


            bleu_score += bleu_results
            sacre_bleu_score += sacre_bleu_result
            gleu_score += gleu_score_result

            rouge1_fmeasure += rouge_result['rouge1_fmeasure']
            rouge1_precision += rouge_result['rouge1_precision']
            rouge1_recall += rouge_result['rouge1_recall']

            rouge2_fmeasure += rouge_result['rouge2_fmeasure']
            rouge2_precision += rouge_result['rouge2_precision']
            rouge2_recall += rouge_result['rouge2_recall']

            rougeL_fmeasure += rouge_result['rougeL_fmeasure']
            rougeL_precision += rouge_result['rougeL_precision']
            rougeL_recall += rouge_result['rougeL_recall']


            # print("Predicted Answer:", predicted_answer)
        if j == 7000:
          break
        j += 1

# bleu_score /= len(dataloader)
# rouge_score /= len(dataloader)
# sacre_bleu_score /= len(dataloader)
# gleu_score /= len(dataloader)

bleu_score /= 7000
sacre_bleu_score /= 7000
gleu_score /= 7000

rouge1_fmeasure /= 7000
rouge1_precision /= 7000
rouge1_recall /= 7000

rouge2_fmeasure /= 7000
rouge2_precision /= 7000
rouge2_recall /= 7000

rougeL_fmeasure /= 7000
rougeL_precision /= 7000
rougeL_recall /= 7000

print(f"BLEU: {bleu_score:.4f}")
print(f"SACRE_BLEU: {sacre_bleu_score:.4f}")
print(f"GLEU: {gleu_score:.4f}")
print()

print(f"ROUGH1 Fmeasure: {rouge1_fmeasure:.4f}")
print(f"ROUGH1 Precision: {rouge1_precision:.4f}")
print(f"ROUGH1 Recall: {rouge1_recall:.4f}")
print()

print(f"ROUGH2 Fmeasure: {rouge2_fmeasure:.4f}")
print(f"ROUGH2 Precision: {rouge2_precision:.4f}")
print(f"ROUGH2 Recall: {rouge2_recall:.4f}")
print()

print(f"ROUGHL Fmeasure: {rougeL_fmeasure:.4f}")
print(f"ROUGHL Precision: {rougeL_precision:.4f}")
print(f"ROUGHL Recall: {rougeL_recall:.4f}")

"""## Text Genration Example"""

question = test_privacy_df.iloc[0]['Query']
context = test_privacy_df.iloc[0]['Segment']

inputs = tokenizer(question, context, return_tensors="pt")
inputs = inputs.to(device)
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()
predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
pred_text = tokenizer.decode(predict_answer_tokens)

print(f"Question: {question}\n")
print("Answer")
print(f"Predition Text: {pred_text}")
print(f"Reference Text: {context}")

"""# PolicyQA Dataset"""

def preprocess_to_csv(file_path, split = 'train'):

  with open(file_path, "r") as f:
    data = json.load(f)

  new_data = []

  for item in data['data']:
      for paragraph in item['paragraphs']:

          context = paragraph['context']

          for qa in paragraph['qas']:
              entry = {
                  'question': qa['question'],
                  'answer': qa['answers'][0]['text'] if qa['answers'] else "NA",
                  'answer_start' : qa['answers'][0]['answer_start'],
                  'answer_end' : qa['answers'][0]['answer_start'] + len(qa['answers'][0]['text']),
                  'context': context,
                  'id': qa['id'],
              }
              new_data.append(entry)

  df = pd.DataFrame(new_data)
  df.to_csv(f"{split}.csv", index=False)
  data = pd.read_csv(f"{split}.csv")

  # dataset = load_dataset("csv", data_files = {"data": f"{split}.csv"})

  return data

test_data = preprocess_to_csv('/content/test.json', split = 'test')

test_data_df = test_data[['question', 'answer']]

test_data_df.info()

test_data_df = test_data_df.drop_duplicates(keep=False)
test_data_df.info()

test_data_df.isnull().sum()

test_data_df.dropna(inplace=True)
test_data_df.isnull().sum()

test_data_df.info()

test_data_df.head()

# mkae datasetclass
class CustomDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=64):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question = self.data.iloc[idx]['question']
        context = self.data.iloc[idx]['answer']

        # Tokenize question and context with padding and truncation
        inputs = self.tokenizer(question, context, return_tensors="pt", padding=True, truncation=True, max_length=self.max_length)

        return inputs, context

# Create an instance of your custom dataset
custom_dataset = CustomDataset(test_data_df, tokenizer)

# make dataloader
BATCH_SIZE = 1
dataloader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

bleu_score = 0
sacre_bleu_score = 0
gleu_score = 0

rouge1_fmeasure = 0
rouge1_precision = 0
rouge1_recall = 0

rouge2_fmeasure = 0
rouge2_precision = 0
rouge2_recall = 0

rougeL_fmeasure = 0
rougeL_precision = 0
rougeL_recall = 0



model = model.to(device)
j = 0
for batch, context in dataloader:
    with torch.no_grad():
        batch['input_ids'] = batch['input_ids'].squeeze(dim=0)
        batch['attention_mask'] = batch['attention_mask'].squeeze(dim=0)

        batch = batch.to(device)
        outputs = model(**batch)
        answer_start_indices = outputs.start_logits.argmax(dim=1)
        answer_end_indices = outputs.end_logits.argmax(dim=1)
        for i in range(BATCH_SIZE):

            start_idx = answer_start_indices[i].item()
            end_idx = answer_end_indices[i].item()
            predict_answer_tokens = batch.input_ids[i, start_idx : end_idx + 1]
            predicted_answer = tokenizer.decode(predict_answer_tokens)


            bleu_results = bleu([predicted_answer], [[context[0]]])
            rouge_result = rouge(predicted_answer, context[0])
            sacre_bleu_result = sacre_bleu([predicted_answer], [[context[0]]])
            gleu_score_result = sentence_gleu(predicted_answer.split(), context[0].split())


            bleu_score += bleu_results
            sacre_bleu_score += sacre_bleu_result
            gleu_score += gleu_score_result

            rouge1_fmeasure += rouge_result['rouge1_fmeasure']
            rouge1_precision += rouge_result['rouge1_precision']
            rouge1_recall += rouge_result['rouge1_recall']

            rouge2_fmeasure += rouge_result['rouge2_fmeasure']
            rouge2_precision += rouge_result['rouge2_precision']
            rouge2_recall += rouge_result['rouge2_recall']

            rougeL_fmeasure += rouge_result['rougeL_fmeasure']
            rougeL_precision += rouge_result['rougeL_precision']
            rougeL_recall += rouge_result['rougeL_recall']


            # print("Predicted Answer:", predicted_answer)
        # if j == 7000:
        #   break
        # j += 1

# bleu_score /= len(dataloader)
# rouge_score /= len(dataloader)
# sacre_bleu_score /= len(dataloader)
# gleu_score /= len(dataloader)

bleu_score /= len(dataloader)
sacre_bleu_score /= len(dataloader)
gleu_score /= len(dataloader)

rouge1_fmeasure /= len(dataloader)
rouge1_precision /= len(dataloader)
rouge1_recall /= len(dataloader)

rouge2_fmeasure /= len(dataloader)
rouge2_precision /= len(dataloader)
rouge2_recall /= len(dataloader)

rougeL_fmeasure /= len(dataloader)
rougeL_precision /= len(dataloader)
rougeL_recall /= len(dataloader)

32print(f"BLEU: {bleu_score:.4f}")
print(f"SACRE_BLEU: {sacre_bleu_score:.4f}")
print(f"GLEU: {gleu_score:.4f}")
print()

print(f"ROUGH1 Fmeasure: {rouge1_fmeasure:.4f}")
print(f"ROUGH1 Precision: {rouge1_precision:.4f}")
print(f"ROUGH1 Recall: {rouge1_recall:.4f}")
print()

print(f"ROUGH2 Fmeasure: {rouge2_fmeasure:.4f}")
print(f"ROUGH2 Precision: {rouge2_precision:.4f}")
print(f"ROUGH2 Recall: {rouge2_recall:.4f}")
print()

print(f"ROUGHL Fmeasure: {rougeL_fmeasure:.4f}")
print(f"ROUGHL Precision: {rougeL_precision:.4f}")
print(f"ROUGHL Recall: {rougeL_recall:.4f}")

"""## Text Genration Example"""

question = test_data_df.iloc[1]['question']
context = test_data_df.iloc[1]['answer']

inputs = tokenizer(question, context, return_tensors="pt")
inputs = inputs.to(device)
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()
predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
pred_text = tokenizer.decode(predict_answer_tokens)

print(f"Question: {question}\n")
print("Answer")
print(f"Predition Text: {pred_text}")
print(f"Reference Text: {context}")